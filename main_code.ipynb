{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ccd713",
   "metadata": {},
   "source": [
    "Calculate link criticality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import networkx as nx  \n",
    "import xml.etree.ElementTree as ET  \n",
    "from heapq import *  \n",
    "from itertools import *  \n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib as mpl\n",
    "import xml.etree.ElementTree as ET\n",
    "import networkx as nx\n",
    "import traceback\n",
    "\n",
    "def parse_sumo_net(net_xml_path):\n",
    "\n",
    "    tree = ET.parse(net_xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for node in root.findall('.//node'):\n",
    "        nid = node.get('id')\n",
    "        G.add_node(nid)\n",
    "\n",
    "\n",
    "    edge_accum = {}  # key=(u,v)\n",
    "\n",
    "    ignored_prefixes = [':', '-']\n",
    "    for edge in root.findall('.//edge'):\n",
    "        eid = edge.get('id')\n",
    "        if any(eid.startswith(pref) for pref in ignored_prefixes):\n",
    "            continue\n",
    "        u, v = edge.get('from'), edge.get('to')\n",
    "        pair = tuple(sorted([u, v]))\n",
    "\n",
    "\n",
    "        length = float(edge.get('length', 0.0))\n",
    "        speed  = float(edge.get('speed', 0.0))\n",
    "        q      = float(edge.get('q', float('inf')))\n",
    "\n",
    "        actual_speeds = []\n",
    "        for lane in edge.findall('lane'):\n",
    "            a = lane.get('actualSpeed')\n",
    "            if a is not None:\n",
    "                try:\n",
    "                    actual_speeds.append(float(a))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        if not actual_speeds:\n",
    "            actual = speed\n",
    "        else:\n",
    "            actual = sum(actual_speeds) / len(actual_speeds)\n",
    "\n",
    "        if pair not in edge_accum:\n",
    "            edge_accum[pair] = {\n",
    "                'length_sum': length,\n",
    "                'speed_sum':  speed,\n",
    "                'actual_sum': actual,\n",
    "                'count':      1,\n",
    "                'q_min':      q\n",
    "            }\n",
    "        else:\n",
    "            rec = edge_accum[pair]\n",
    "            rec['length_sum'] += length\n",
    "            rec['speed_sum']  += speed\n",
    "            rec['actual_sum'] += actual\n",
    "            rec['count']     += 1\n",
    "            rec['q_min'] += q\n",
    "            # if q < rec['q_min']:\n",
    "            #     rec['q_min'] = q\n",
    "\n",
    "    for (u, v), rec in edge_accum.items():\n",
    "        cnt = rec['count']\n",
    "        G.add_edge(\n",
    "            u, v,\n",
    "            length      = rec['length_sum'] / cnt,\n",
    "            speed       = rec['speed_sum']  / cnt,\n",
    "            actualSpeed = rec['actual_sum'] / cnt,\n",
    "            q           = rec['q_min']  / cnt\n",
    "        )\n",
    "\n",
    "    print(f\"Total nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Total edges (after aggregation): {G.number_of_edges()}\")\n",
    "\n",
    "    return G \n",
    "\n",
    "def maximum_capacity_paths(G, source, weight, target=None):  \n",
    "\n",
    "    get_weight = lambda u, v, data: data.get(weight, 1)  \n",
    "    paths = {source: [source]}  \n",
    "    G_succ = G.succ if G.is_directed() else G.adj  \n",
    "\n",
    "    push = heappush  \n",
    "    pop = heappop  \n",
    "    dist = {}  \n",
    "    pred = {}  \n",
    "    seen = {source: 0}  \n",
    "    c = count()  \n",
    "    fringe = []  \n",
    "    push(fringe, (-1, next(c), source))  \n",
    "    while fringe:  \n",
    "        (d, _, v) = pop(fringe)  \n",
    "        if v in dist:  \n",
    "            continue  \n",
    "        dist[v] = d  \n",
    "        if v == target:  \n",
    "            break  \n",
    "        for u, e in G_succ[v].items():  \n",
    "            if u == source:  \n",
    "                continue  \n",
    "            vu_dist = max([dist[v], -get_weight(v, u, e)])  \n",
    "            if u not in seen or vu_dist < seen[u]:  \n",
    "                seen[u] = vu_dist  \n",
    "                push(fringe, (vu_dist, next(c), u))  \n",
    "                if paths is not None:  \n",
    "                    paths[u] = paths[v] + [u]  \n",
    "\n",
    "                if dist[v] > -get_weight(v, u, e):  \n",
    "                    pred[u] = pred[v]  \n",
    "                else:  \n",
    "                    pred[u] = (v, u)  \n",
    "\n",
    "    dist = {i: -dist[i] for i in dist}  \n",
    "    return dist, pred   \n",
    "\n",
    "def canonical(u, v):\n",
    "    return (u, v) if u <= v else (v, u)\n",
    "\n",
    "def analyze_network_percolation(net, true_OD, total_demand):   \n",
    "    net_nodes = set(map(str, net.nodes())) \n",
    "    true_OD_nodes = {tuple(map(str, od)) for od in true_OD.keys()}  \n",
    "\n",
    "\n",
    "    converted_true_OD = {}  \n",
    "    for (orig, dest), demand in true_OD.items():  \n",
    "        orig_str = str(orig)  \n",
    "        dest_str = str(dest)  \n",
    "        converted_true_OD[(orig_str, dest_str)] = demand  \n",
    "\n",
    "    unmatched_orig = set()  \n",
    "    unmatched_dest = set()  \n",
    "    matched_od = 0  \n",
    "\n",
    "    for (orig, dest) in converted_true_OD.keys():  \n",
    "        if orig not in net_nodes:  \n",
    "            unmatched_orig.add(orig)  \n",
    "        if dest not in net_nodes:  \n",
    "            unmatched_dest.add(dest)  \n",
    "        if orig in net_nodes and dest in net_nodes:  \n",
    "            matched_od += 1  \n",
    "\n",
    "    valid_true_OD = {  \n",
    "        (orig, dest): demand   \n",
    "        for (orig, dest), demand in converted_true_OD.items()   \n",
    "        if orig in net_nodes and dest in net_nodes  \n",
    "    }  \n",
    "\n",
    "    criticalityScores = {}  \n",
    "    counted_od = set()  \n",
    "    od_result = 0  \n",
    "\n",
    "    for Orig in net_nodes:  \n",
    "        _, limitingLinksDict = maximum_capacity_paths(net, Orig, weight='q')  \n",
    "        \n",
    "        for Dest, raw_link in limitingLinksDict.items():  \n",
    "\n",
    "            Dest = str(Dest)  \n",
    "            \n",
    "            if Orig != Dest and (Orig, Dest) in valid_true_OD:  \n",
    "                link = canonical(raw_link[0], raw_link[1])   \n",
    "                \n",
    "                criticalityScores.setdefault(link, 0)  \n",
    "                criticalityScores[link] += valid_true_OD[(Orig, Dest)] / float(total_demand)  \n",
    "                od_result += valid_true_OD[(Orig, Dest)]  \n",
    "                counted_od.add((Orig, Dest))  \n",
    "\n",
    "    linkInfoList = []\n",
    "    for u, v in net.edges():\n",
    "        key = canonical(u, v)\n",
    "        q = float(net[u][v]['q'])\n",
    "        score = criticalityScores.get(key, 0)\n",
    "        linkInfoList.append((u, v, q, score))\n",
    "\n",
    "    alpha = np.sum([q * score for (_, _, q, score) in linkInfoList])\n",
    "\n",
    "    rhos = [rho/1000.0 for rho in range(1001)]\n",
    "\n",
    "    y_vals = [\n",
    "    sum(s for (_,_,q,s) in linkInfoList if q >= rho)\n",
    "    if abs(rho - 1.0) < 1e-9\n",
    "    else sum(s for (_,_,q,s) in linkInfoList if q > rho)\n",
    "    for rho in rhos]\n",
    "\n",
    "    return linkInfoList, alpha, od_result\n",
    "\n",
    "\n",
    "def plot_ud_alpha(net, link_info):\n",
    "    rho_values = [rho/1000.0 for rho in range(1001)]\n",
    "\n",
    "    unaffected_demand = [np.sum([link[3] for link in link_info if link[2] > rho]) for rho in rho_values]\n",
    "    total_nodes = net.number_of_nodes()  \n",
    "\n",
    "    largest_cc_sizes = []\n",
    "    second_largest_cc_sizes = []\n",
    "    for rho in rho_values:\n",
    "\n",
    "        H = nx.Graph()\n",
    "        H.add_nodes_from(net.nodes())\n",
    "        for u, v, data in net.edges(data=True):\n",
    "            if data['q'] > rho:\n",
    "                H.add_edge(u, v)\n",
    "\n",
    "        components = nx.connected_components(H)\n",
    "\n",
    "        if components:\n",
    "            comps_sorted = sorted(components, key=lambda comp: len(comp), reverse=True)\n",
    "            largest_size = len(comps_sorted[0])\n",
    "\n",
    "            second_largest_size = len(comps_sorted[1]) if len(comps_sorted) > 1 else 0\n",
    "        else:\n",
    "            largest_size = 0\n",
    "            second_largest_size = 0\n",
    "        largest_cc_sizes.append(largest_size / total_nodes)\n",
    "        second_largest_cc_sizes.append(second_largest_size / total_nodes)\n",
    "\n",
    "    \n",
    "    idx_max2 = int(np.argmax(second_largest_cc_sizes))\n",
    "    rho_max2 = rho_values[idx_max2]\n",
    "\n",
    "    return rho_max2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e896f6",
   "metadata": {},
   "source": [
    "Make network weighted matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Here are the associated DomiRank functions #############\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "########## Here are the general functions needed for efficient dismantling and testing of networks #############\n",
    "\n",
    "def get_largest_component(G, strong = False):\n",
    "    '''\n",
    "    here we get the largest component of a graph, either from scipy.sparse or from networkX.Graph datatype.\n",
    "    1. The argument changes whether or not you want to find the strong or weak - connected components of the graph'''\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        if nx.is_directed(G) and strong == False:\n",
    "            GMask = max(nx.weakly_connected_components(G), key = len)\n",
    "        if nx.is_directed(G) and strong == True:\n",
    "            GMask = max(nx.strongly_connected_components(G), key = len)\n",
    "        else:\n",
    "            GMask = max(nx.connected_components(G), key = len)\n",
    "        G = G.subgraph(GMask)\n",
    "    else:\n",
    "        raise TypeError('You must input a networkx.Graph Data-Type')\n",
    "    return G\n",
    "\n",
    "def relabel_nodes(G, yield_map = False):\n",
    "    '''relabels the nodes to be from 0, ... len(G).\n",
    "    1. Yield_map returns an extra output as a dict. in case you want to save the hash-map to retrieve node-id'''\n",
    "    if yield_map == True:\n",
    "        nodes = dict(zip(range(len(G)), G.nodes()))\n",
    "        G = nx.relabel_nodes(G, dict(zip(G.nodes(), range(len(G)))))\n",
    "        return G, nodes\n",
    "    else:\n",
    "        G = nx.relabel_nodes(G, dict(zip(G.nodes(), range(len(G)))))\n",
    "        return G\n",
    "\n",
    "def get_component_size(G, strong = False):\n",
    "    '''\n",
    "    here we get the largest component of a graph, either from scipy.sparse or from networkX.Graph datatype.\n",
    "    1. The argument changes whether or not you want to find the strong or weak - connected components of the graph'''\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        if nx.is_directed(G) and strong == False:\n",
    "            GMask = max(nx.weakly_connected_components(G), key = len)\n",
    "        if nx.is_directed(G) and strong == True:\n",
    "            GMask = max(nx.strongly_connected_components(G), key = len)\n",
    "        else:\n",
    "            GMask = max(nx.connected_components(G), key = len)\n",
    "        G = G.subgraph(GMask)\n",
    "        return len(GMask)        \n",
    "    elif type(G) == scipy.sparse.csr_array:\n",
    "        if strong == False:\n",
    "            connection_type = 'weak'\n",
    "        else:\n",
    "            connection_type = 'strong'\n",
    "        noComponent, lenComponent = sp.sparse.csgraph.connected_components(G, directed = True, connection = connection_type, return_labels = True)\n",
    "        return np.bincount(lenComponent).max()\n",
    "    else:\n",
    "        raise TypeError('You must input a networkx.Graph Data-Type or scipy.sparse.csr array')\n",
    "        \n",
    "def get_link_size(G):\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        links = len(G.edges()) #convert to scipy sparse if it is a graph \n",
    "    elif type(G) == scipy.sparse.csr_array:\n",
    "        links = G.sum()\n",
    "    else:\n",
    "        raise TypeError('You must input a networkx.Graph Data-Type')\n",
    "    return links\n",
    "\n",
    "def remove_node(G, removedNode):\n",
    "    '''\n",
    "    removes the node from the graph by removing it from a networkx.Graph type, or zeroing the edges in array form.\n",
    "    '''\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        if type(removedNode) == int:\n",
    "            G.remove_node(removedNode)\n",
    "        else:\n",
    "            for node in removedNode:\n",
    "                G.remove_node(node) #remove node in graph form\n",
    "        return G\n",
    "    elif type(G) == scipy.sparse.csr_array:\n",
    "        diag = sp.sparse.csr_array(sp.sparse.eye(G.shape[0])) \n",
    "        diag[removedNode, removedNode] = 0 #set the rows and columns that are equal to zero in the sparse array\n",
    "        G = diag @ G \n",
    "        return G @ diag\n",
    "    \n",
    "def generate_attack(centrality, node_map = False):\n",
    "    '''we generate an attack based on a centrality measure - \n",
    "    you can possibly input the node_map to convert the attack to have the correct nodeID'''\n",
    "    if node_map == False:\n",
    "        node_map = range(len(centrality))\n",
    "    else:\n",
    "        node_map = list(node_map.values())\n",
    "    zipped = dict(zip(node_map, centrality))\n",
    "    attackStrategy = sorted(zipped, reverse = True, key = zipped.get)\n",
    "    return attackStrategy\n",
    "\n",
    "def generate_attack_our(centrality, node_map = False):\n",
    "    '''we generate an attack based on a centrality measure - \n",
    "    you can possibly input the node_map to convert the attack to have the correct nodeID'''\n",
    "    if node_map == False:\n",
    "        node_map = range(len(centrality))\n",
    "    else:\n",
    "        node_map = list(node_map.values())\n",
    "    zipped = dict(zip(node_map, centrality))\n",
    "    attackStrategy = sorted(zipped, reverse = True, key = zipped.get)\n",
    "    return attackStrategy\n",
    "\n",
    "def network_attack_sampled(G, attackStrategy, sampling = 0):\n",
    "    '''Attack a network in a sampled manner... recompute links and largest component after every xth node removal, according to some - \n",
    "    G: is the input graph, preferably as a sparse array.\n",
    "    inputed attack strategy\n",
    "    Note: if sampling is not set, it defaults to sampling every 1%, otherwise, sampling is an integer\n",
    "    that is equal to the number of nodes you want to skip every time you sample. \n",
    "    So for example sampling = int(len(G)/100) would sample every 1% of the nodes removed'''\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        GAdj = nx.to_scipy_sparse_array(G) #convert to scipy sparse if it is a graph \n",
    "    else:\n",
    "        GAdj = G.copy()\n",
    "    \n",
    "    if sampling == 0:\n",
    "        if GAdj.shape[0] < 100:\n",
    "            sampling = 1\n",
    "        else:\n",
    "            sampling = int(GAdj.shape[0]/100)\n",
    "    N = GAdj.shape[0]\n",
    "    initialComponent = get_component_size(GAdj)\n",
    "    initialLinks = get_link_size(GAdj)\n",
    "    m = GAdj.sum()/N\n",
    "    componentEvolution = np.zeros((N//sampling + 1))\n",
    "    linksEvolution = np.zeros((N//sampling) + 1)\n",
    "    j = 0 \n",
    "    for i in range(N-1):\n",
    "        if i % sampling == 0:\n",
    "            if i == 0:\n",
    "                componentEvolution[j] = get_component_size(GAdj)/initialComponent\n",
    "                linksEvolution[j] = get_link_size(GAdj)/initialLinks\n",
    "                j+=1 \n",
    "            else:\n",
    "                GAdj = remove_node(GAdj, attackStrategy[i-sampling:i])\n",
    "                componentEvolution[j] = get_component_size(GAdj)/initialComponent\n",
    "                linksEvolution[j] = get_link_size(GAdj)/initialLinks\n",
    "                j+=1\n",
    "    return componentEvolution, linksEvolution\n",
    "\n",
    "\n",
    "\n",
    "######## Beginning of domirank stuff! ####################\n",
    "\n",
    "def domirank(G, analytical = True, sigma = -1, dt = 0.1, epsilon = 1e-5, maxIter = 1000, checkStep = 10):\n",
    "    '''\n",
    "    G is the input graph as a (preferably) sparse array.\n",
    "    This solves the dynamical equation presented in the Paper: \"DomiRank Centrality: revealing structural fragility of\n",
    "complex networks via node dominance\" and yields the following output: bool, DomiRankCentrality\n",
    "    Here, sigma needs to be chosen a priori.\n",
    "    dt determines the step size, usually, 0.1 is sufficiently fine for most networks (could cause issues for networks\n",
    "    with an extremely high degree, but has never failed me!)\n",
    "    maxIter is the depth that you are searching with in case you don't converge or diverge before that.\n",
    "    Checkstep is the amount of steps that you go before checking if you have converged or diverged.\n",
    "    \n",
    "    \n",
    "    This algorithm scales with O(m) where m is the links in your sparse array.\n",
    "    '''\n",
    "    if type(G) == nx.classes.graph.Graph: #check if it is a networkx Graph\n",
    "        G = nx.to_scipy_sparse_array(G) #convert to scipy sparse if it is a graph \n",
    "    else:\n",
    "        G = G.copy()\n",
    "    if analytical == False:\n",
    "\n",
    "        if sigma == -1:\n",
    "            sigma, _ = optimal_sigma(G, analytical = False, dt=dt, epsilon=epsilon, maxIter = maxIter, checkStep = checkStep) \n",
    "        pGAdj = sigma*G.astype(np.float64)\n",
    "        Psi = np.ones(pGAdj.shape[0]).astype(np.float64)/pGAdj.shape[0]\n",
    "        maxVals = np.zeros(int(maxIter/checkStep)).astype(np.float64)\n",
    "        dt = np.float64(dt)\n",
    "        j = 0\n",
    "        boundary = epsilon*pGAdj.shape[0]*dt\n",
    "        for i in range(maxIter):\n",
    "            tempVal = ((pGAdj @ (1-Psi)) - Psi)*dt\n",
    "            Psi += tempVal.real\n",
    "            if i% checkStep == 0:\n",
    "                if np.abs(tempVal).sum() < boundary:\n",
    "                    break\n",
    "                maxVals[j] = tempVal.max()\n",
    "                if i == 0:\n",
    "                    initialChange = maxVals[j]\n",
    "                if j > 0:\n",
    "                    if maxVals[j] > maxVals[j-1] and maxVals[j-1] > maxVals[j-2]:\n",
    "                        return False, Psi\n",
    "                j+=1\n",
    "\n",
    "        return True, Psi\n",
    "    else:\n",
    "\n",
    "        if sigma == -1:\n",
    "            sigma = optimal_sigma(G, analytical = True, dt=dt, epsilon=epsilon, maxIter = maxIter, checkStep = checkStep) \n",
    "        Psi = sp.sparse.linalg.spsolve(sigma*G + sp.sparse.identity(G.shape[0]), sigma*G.sum(axis=-1))\n",
    "        return True, Psi\n",
    "    \n",
    "def find_eigenvalue(G, minVal = 0, maxVal = 1, maxDepth = 100, dt = 0.1, epsilon = 1e-5, maxIter = 100, checkStep = 10):\n",
    "    '''\n",
    "    G: is the input graph as a sparse array.\n",
    "    Finds the largest negative eigenvalue of an adjacency matrix using the DomiRank algorithm.\n",
    "    Currently this function is only single-threaded, as the bisection algorithm only allows for single-threaded\n",
    "    exection. Note, that this algorithm is slightly different, as it uses the fact that DomiRank diverges\n",
    "    at values larger than -1/lambN to its benefit, and thus, it is not exactly bisection theorem. I haven't\n",
    "    tested in order to see which exact value is the fastest for execution, but that will be done soon!\n",
    "    Some notes:\n",
    "    Increase maxDepth for increased accuracy.\n",
    "    Increase maxIter if DomiRank doesn't start diverging within 100 iterations -- i.e. increase at the expense of \n",
    "    increased computational cost if you want potential increased accuracy.\n",
    "    Decrease checkstep for increased error-finding for the values of sigma that are too large, but higher compcost\n",
    "    if you are frequently less than the value (but negligible compcost).\n",
    "    '''\n",
    "    x = (minVal + maxVal)/G.sum(axis=-1).max()\n",
    "    minValStored = 0\n",
    "    for i in range(maxDepth):\n",
    "        if maxVal - minVal < epsilon:\n",
    "            break\n",
    "        if domirank(G, False, x, dt, epsilon, maxIter, checkStep)[0]:\n",
    "            minVal = x\n",
    "            x = (minVal + maxVal)/2\n",
    "            minValStored = minVal\n",
    "        else:\n",
    "            maxVal = (x + maxVal)/2\n",
    "            x = (minVal + maxVal)/2\n",
    "        if minVal == 0:\n",
    "            print(f'Current Interval : [-inf, -{1/maxVal}]')\n",
    "        else:\n",
    "            print(f'Current Interval : [-{1/minVal}, -{1/maxVal}]')\n",
    "    finalVal = (maxVal + minVal)/2\n",
    "    return -1/finalVal\n",
    "\n",
    "\n",
    "\n",
    "############## This section is for finding the optimal sigma #######################\n",
    "def process_iteration_wrapper(i, sigma, analytical, spArray, maxIter, checkStep, dt, epsilon, sampling):\n",
    "\n",
    "    tf, domiDist = domirank(spArray, analytical=analytical, sigma=sigma, dt=dt, epsilon=epsilon, maxIter=maxIter, checkStep=checkStep)\n",
    "\n",
    "    domiAttack = generate_attack(domiDist)\n",
    "\n",
    "    ourTempAttack, _ = network_attack_sampled(spArray, domiAttack, sampling=sampling)\n",
    "    finalErrors = ourTempAttack.sum()\n",
    "    return (i, finalErrors)\n",
    "\n",
    "def optimal_sigma(spArray, analytical=True, endVal=0, startval=1e-6, iterationNo=100,\n",
    "                  dt=0.1, epsilon=1e-5, maxIter=100, checkStep=10, maxDepth=100, sampling=0):\n",
    "    \n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "    if endVal == 0:\n",
    "        endVal = find_eigenvalue(spArray, maxDepth=maxDepth, dt=dt, epsilon=epsilon, maxIter=maxIter, checkStep=checkStep)\n",
    "\n",
    "    endval = -0.9999 / endVal\n",
    "\n",
    "    tempRange = np.linspace(startval, endval, iterationNo)\n",
    "    \n",
    "\n",
    "    args = [(i, sigma, analytical, spArray, maxIter, checkStep, dt, epsilon, sampling)\n",
    "            for i, sigma in enumerate(tempRange)]\n",
    "\n",
    "    with ThreadPool() as pool:\n",
    "        results = pool.starmap(process_iteration_wrapper, args)\n",
    "\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    finalErrors = np.array([r[1] for r in results])\n",
    "    best_index = np.argmin(finalErrors)\n",
    "    best_sigma = tempRange[best_index]\n",
    "    return best_sigma, finalErrors, tempRange\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def network_attack_metrics(linkInfoList, attackStrategy, sampling=0):\n",
    "\n",
    "    N = len(attackStrategy)\n",
    "    if sampling == 0:\n",
    "        sampling = 1 if N < 100 else max(1, N // 100)\n",
    "\n",
    "\n",
    "    total_initial = sum(score for _, _, _, score in linkInfoList)\n",
    "\n",
    "    G0 = nx.Graph()\n",
    "    for u, v, _, _ in linkInfoList:\n",
    "        G0.add_edge(u, v)\n",
    "    initial_lcc = len(max(nx.connected_components(G0), key=len))\n",
    "\n",
    "    num_samples = N // sampling + 1\n",
    "    ua_curve = np.zeros(num_samples, dtype=float)\n",
    "    lcc_curve = np.zeros(num_samples, dtype=float)\n",
    "    frac_removed = np.zeros(num_samples, dtype=float)\n",
    "\n",
    "    removed = set()\n",
    "    idx = 0\n",
    "\n",
    "    ua_curve[idx] = total_initial\n",
    "    lcc_curve[idx] = initial_lcc\n",
    "    frac_removed[idx] = 0.0\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "    for i, node in enumerate(attackStrategy, start=1):\n",
    "        removed.add(node)\n",
    "        if i % sampling == 0:\n",
    " \n",
    "            ua = sum(score for u, v, _, score in linkInfoList\n",
    "                     if u not in removed and v not in removed)\n",
    "            ua_curve[idx] = ua\n",
    "\n",
    "            G = G0.copy()\n",
    "            G.remove_nodes_from(removed)\n",
    "            if G.number_of_nodes() > 0:\n",
    "                lcc_curve[idx] = len(max(nx.connected_components(G), key=len))\n",
    "            else:\n",
    "                lcc_curve[idx] = 0\n",
    "   \n",
    "            frac_removed[idx] = i / float(N)\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "    ua_curve = ua_curve[:idx]\n",
    "    lcc_curve = lcc_curve[:idx]\n",
    "    frac_removed = frac_removed[:idx]\n",
    "\n",
    "    return frac_removed, ua_curve, lcc_curve\n",
    "def resilience_attack_metrics(linkInfoList, attackStrategy, sampling=0):\n",
    " \n",
    "    N = len(attackStrategy)\n",
    "    if sampling == 0:\n",
    "        sampling = 1 if N < 100 else max(1, N // 100)\n",
    "\n",
    "    total_initial = sum(q*score for _, _, q, score in linkInfoList)\n",
    "\n",
    "    G0 = nx.Graph()\n",
    "    for u, v, _, _ in linkInfoList:\n",
    "        G0.add_edge(u, v)\n",
    "    initial_lcc = len(max(nx.connected_components(G0), key=len))\n",
    "\n",
    "    num_samples = N // sampling + 1\n",
    "    resilience_curve = np.zeros(num_samples, dtype=float)\n",
    "    lcc_curve = np.zeros(num_samples, dtype=float)\n",
    "    frac_removed = np.zeros(num_samples, dtype=float)\n",
    "\n",
    "    removed = set()\n",
    "    idx = 0\n",
    "\n",
    "\n",
    "    resilience_curve[idx] = total_initial\n",
    "    lcc_curve[idx] = initial_lcc\n",
    "    frac_removed[idx] = 0.0\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "    for i, node in enumerate(attackStrategy, start=1):\n",
    "        removed.add(node)\n",
    "        if i % sampling == 0:\n",
    "    \n",
    "            re = sum(q*score for u, v, q, score in linkInfoList\n",
    "                     if u not in removed and v not in removed)\n",
    "            resilience_curve[idx] = re\n",
    "        \n",
    "            G = G0.copy()\n",
    "            G.remove_nodes_from(removed)\n",
    "            if G.number_of_nodes() > 0:\n",
    "                lcc_curve[idx] = len(max(nx.connected_components(G), key=len))\n",
    "            else:\n",
    "                lcc_curve[idx] = 0\n",
    "     \n",
    "            frac_removed[idx] = i / float(N)\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "    resilience_curve = resilience_curve[:idx]\n",
    "    lcc_curve = lcc_curve[:idx]\n",
    "    frac_removed = frac_removed[:idx]\n",
    "\n",
    "    return frac_removed, resilience_curve, lcc_curve\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def network_attack_unaffected(linkInfoList, attackStrategy, sampling=0):\n",
    "\n",
    "    N = len(attackStrategy)\n",
    "\n",
    "    if sampling == 0:\n",
    "        sampling = 1 if N < 100 else N // 100\n",
    "\n",
    "\n",
    "    total_initial = sum(score for _, _, _, score in linkInfoList)\n",
    "\n",
    "    num_samples = N // sampling + 1\n",
    "    unaffected = np.zeros(num_samples, dtype=float)\n",
    "\n",
    "    removed = set()\n",
    "    idx = 0\n",
    "\n",
    "\n",
    "    unaffected[idx] = total_initial\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "    for i, node in enumerate(attackStrategy, start=1):\n",
    "        removed.add(node)\n",
    "\n",
    "  \n",
    "        if i % sampling == 0:\n",
    "            s = 0.0\n",
    "          \n",
    "            for u, v, _, score in linkInfoList:\n",
    "                if u not in removed and v not in removed:\n",
    "                    s += score\n",
    "            unaffected[idx] = s\n",
    "            idx += 1\n",
    "\n",
    "    return unaffected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed3f0d",
   "metadata": {},
   "source": [
    "Identification of resilient-node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_array\n",
    "\n",
    "    \n",
    "def compute_domirank(GAdj, net, directed=False, analytical=False,\n",
    "                     max_iter=1000, dt=0.01, check_step=25):\n",
    "\n",
    "    \n",
    "    analytical = False #if you want to use the analytical method or the recursive definition\n",
    "    directed = False\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ##### #RANDOMIZATION ######\n",
    "    \n",
    "    #for random results\n",
    "    seed = np.random.randint(0, high = 2**30-1)\n",
    "    \n",
    "    #for deterministic results\n",
    "    #seed = 42\n",
    "    \n",
    "    #setting the random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    ##### END OF RANDOMIZATION #####\n",
    "    \n",
    "    ############## IMPORTANT!!!! Here you can create whatever graph you want and just comment this erdos-renyi network out ############\n",
    "    #G = nx.fast_gnp_random_graph(N, 2*m/N, seed = seed, directed = directed) #####THIS IS THE INPUT, CHANGE THIS TO ANY GRAPH #######\n",
    "\n",
    "    \n",
    "    N = len(net)\n",
    "\n",
    "    if directed:\n",
    "        GAdj = sp.sparse.csr_array(GAdj.T)\n",
    "\n",
    "\n",
    "    G, node_map = relabel_nodes(net, yield_map=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    lambN = find_eigenvalue(GAdj, maxIter=max_iter, dt=dt, checkStep=check_step)\n",
    "    t2 = time.time()\n",
    "    print(f\"\\nFound smallest eigenvalue λ_N = {lambN:.6f} (took {t2-t1:.2f}s)\")\n",
    "\n",
    "\n",
    "    sigma, sigma_array, sigma_area = optimal_sigma(\n",
    "        GAdj, analytical=analytical, endVal=lambN\n",
    "    )\n",
    "    print(f\"Optimal σ = {sigma * -lambN:.6f} / -λ_N\")\n",
    "\n",
    "    _, domi_dist = domirank(GAdj, analytical=analytical, sigma=sigma)\n",
    "\n",
    "    return sigma, sigma_array, sigma_area, domi_dist, node_map\n",
    "\n",
    "def compute_attack_metrics(link_info, domi_dist, node_map):\n",
    " \n",
    "    attack_strategy = generate_attack(domi_dist, node_map)\n",
    "    rho, ua_curve, lcc_curve = resilience_attack_metrics(link_info, attack_strategy)\n",
    "\n",
    "\n",
    "    ua_frac = ua_curve / ua_curve[0]\n",
    "    lcc_frac = lcc_curve / lcc_curve[0]\n",
    "\n",
    "\n",
    "    area_ud  = np.trapz(ua_frac, rho)\n",
    "    area_lcc = np.trapz(lcc_frac, rho)\n",
    "   \n",
    "    return rho, ua_curve, lcc_curve, area_ud, area_lcc, attack_strategy\n",
    "\n",
    "def plot_domirank_curve(sigma_area, sigma_array, sigma):\n",
    "\n",
    "    idx = np.where(sigma_array == sigma_array.min())[0][-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(sigma_area, sigma_array, '-k', label='Area curve')\n",
    "    ax.plot(\n",
    "        sigma_area[idx],\n",
    "        sigma_array[idx],\n",
    "        'ro',\n",
    "        mfc='none',\n",
    "        markersize=8,\n",
    "        label=rf'Opt $\\sigma$={sigma:.3f}'\n",
    "    )\n",
    "    ax.set_xlabel(r'$\\sigma$')\n",
    "    ax.set_ylabel('Area under LCC curve')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_attack_curves(rho, ua_curve, lcc_curve):\n",
    "\n",
    "    ua_frac  = ua_curve  / ua_curve[0]\n",
    "    lcc_frac = lcc_curve / lcc_curve[0]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "    ax1.plot(rho, ua_frac, '-o')\n",
    "    ax1.set_ylabel('Unaffected Demand (fraction)')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(rho, lcc_frac, '-o', color='orange')\n",
    "    ax2.set_xlabel('Fraction of nodes removed')\n",
    "    ax2.set_ylabel('LCC size (fraction)')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_results_to_csv(a_list, ud_auc, lcc_auc, knee_a, filename):\n",
    "\n",
    "    a_list = np.array(a_list)\n",
    "    ud_auc = np.array(ud_auc)\n",
    "    lcc_auc = np.array(lcc_auc)\n",
    "    knee_a = np.array(knee_a)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'a_list': a_list,\n",
    "        'ud_auc': ud_auc,\n",
    "        'lcc_auc': lcc_auc,\n",
    "        'knee_a':knee_a\n",
    "    })\n",
    "    \n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    return df, filename\n",
    "def save_curves_to_csv(rho, ua_curve, lcc_curve, filename):\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'rho':rho,\n",
    "        'ua_curve': ua_curve,\n",
    "        'lcc_curve': lcc_curve\n",
    "    })\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "def save_attack_strategy_to_csv(attack_strategy, filename):\n",
    "\n",
    "    attack_strategy = np.array(attack_strategy)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'attack_strategy': attack_strategy\n",
    "    }) \n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_od_counts(input_file='node_od_counts.csv'):\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    try:\n",
    "\n",
    "        od_df = pd.read_csv(input_file)\n",
    "        \n",
    "\n",
    "        od_counts = {\n",
    "            (row['from_node'], row['to_node']): row['count'] \n",
    "            for _, row in od_df.iterrows()\n",
    "        }    \n",
    "        total_trips = sum(od_counts.values())      \n",
    "        return od_counts\n",
    "\n",
    "\n",
    "def load_curves_from_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    rho = df['rho'].to_numpy()\n",
    "    ua_curve = df['ua_curve'].to_numpy()\n",
    "    lcc_curve = df['lcc_curve'].to_numpy()\n",
    "    return ua_curve, lcc_curve\n",
    "\n",
    "def load_attack_strategy_from_csv(filename):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    attack_strategy = df['attack_strategy'].tolist()\n",
    "\n",
    "    return attack_strategy\n",
    "\n",
    "def process_node_dominance_and_od(link_info, ourDomiRankAttack, ourDomiRankDistribution, node_map, df_od):\n",
    "\n",
    "    matched_keys = []\n",
    "    for id_to_match in ourDomiRankAttack:  \n",
    "        matched_key = [key for key, value in node_map.items() if value == id_to_match]  \n",
    "        \n",
    "        if matched_key:  \n",
    "            matched_keys.append(matched_key[0])  \n",
    "    domirank_dict = {node: ourDomiRankDistribution[node] for node in matched_keys}\n",
    "    \n",
    "    domi_df = pd.DataFrame(list(domirank_dict.items()), columns=[\"node_id\", \"DomiRank\"])\n",
    "    domi_df['node_name'] = domi_df['node_id'].map(node_map)\n",
    "    link_df = pd.DataFrame(link_info, columns=['from','to','q','score'])\n",
    "    endpoint_scores = pd.concat([\n",
    "        link_df[['from','score']].rename(columns={'from':'node'}),\n",
    "        link_df[['to'  ,'score']].rename(columns={'to'  :'node'})\n",
    "    ], ignore_index=True) \\\n",
    "     .groupby('node', as_index=True)['score'] \\\n",
    "     .sum()\n",
    "    domi_df['node_score'] = domi_df['node_name'].astype(str).map(endpoint_scores).fillna(0)\n",
    "\n",
    "    domi_df.sort_values(by='DomiRank', ascending=False, inplace=True)\n",
    "\n",
    "    df_od['id'] = df_od['id'].astype(str)\n",
    "    domi_merged_df = domi_df.merge(df_od, left_on='node_name', right_on='id', how='left')\n",
    "    \n",
    "    return domi_merged_df\n",
    "    \n",
    "def main(GAdj, net, link_info, filename_curve, filename_attack, filename_dfod, df_od, directed=False, analytical=False):\n",
    "    sigma, sigma_array, sigma_area, domi_dist, node_map = compute_domirank(\n",
    "        GAdj, net, directed, analytical\n",
    "    )\n",
    "    plot_domirank_curve(sigma_area, sigma_array, sigma)\n",
    "\n",
    "    rho, ua_curve, lcc_curve, area_ud, area_lcc, attack_strategy = compute_attack_metrics(\n",
    "        link_info, domi_dist, node_map\n",
    "    )\n",
    "    plot_attack_curves(rho, ua_curve, lcc_curve)\n",
    "    domi_merged_df = process_node_dominance_and_od(link_info, attack_strategy, domi_dist, node_map, df_od)\n",
    "    domi_merged_df.to_csv(filename_dfod, index=False)\n",
    "    \n",
    "    save_curves_to_csv(rho, ua_curve, lcc_curve, filename_curve)\n",
    "\n",
    "    save_attack_strategy_to_csv(attack_strategy, filename_attack)\n",
    "\n",
    "    return sigma, area_ud, area_lcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094968f",
   "metadata": {},
   "source": [
    "Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "time_periods = ['6-7', '7-8', '8-9', '9-10', '10-11', '11-12', \n",
    "                '12-13', '13-14', '14-15', '15-16', '16-17', \n",
    "                '17-18', '18-19', '19-20', '20-21', '21-22']\n",
    "\n",
    "base_path = r'E:\\SUMO\\Paper1-data\\research_data\\29May_Thur'\n",
    "\n",
    "# 固定随机种子\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "analytical = False #if you want to use the analytical method or the recursive definition\n",
    "directed = False\n",
    "\n",
    "result_dict = {\n",
    "    'alpha':[],\n",
    "    'rho':[],\n",
    "    'ud': [],\n",
    "    'lcc': [],\n",
    "    'knee': [],\n",
    "    'sigma': []\n",
    "}\n",
    "for period in time_periods:\n",
    "    net_xml_path = f'{base_path}\\\\{period}\\\\enhenced_topology.net.xml'\n",
    "    net = parse_sumo_net(net_xml_path)\n",
    "    od_counts_path = f'{base_path}\\\\{period}\\\\node_od_counts.csv'\n",
    "    od_counts = load_od_counts(od_counts_path)\n",
    "    total_count = len(od_counts)\n",
    "    \n",
    "    df_od = od_counts_to_dataframe(od_counts)\n",
    "    # 2.2 读取knee_a\n",
    "    pareto_csv = os.path.join(base_path, period, 'pareto_param.csv')\n",
    "    if not os.path.isfile(pareto_csv):\n",
    "        raise FileNotFoundError(f\"Missing file: {pareto_csv}\")\n",
    "    pareto_df = pd.read_csv(pareto_csv)\n",
    "\n",
    "    link_info, alpha, missing, od_result = analyze_network_percolation(net, od_counts, total_count)\n",
    "    \n",
    "    print(\"Top 10 Critical Links:\")\n",
    "    top_links = sorted(link_info, key=lambda x: x[3], reverse=True)[:10]\n",
    "    for link in top_links:\n",
    "        print(f\"Link {link[0]}->{link[1]}: q={link[2]:.4f}, Criticality={link[3]:.4f}\")\n",
    "    \n",
    "    rho = plot_ud_alpha(net, link_info)\n",
    "    node_list = list(net.nodes())\n",
    "    A = nx.to_scipy_sparse_array(net)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for u, v, q, score in link_info:\n",
    "        if score > 0:\n",
    "            i = node_list.index(u)\n",
    "            j = node_list.index(v)\n",
    "            rows.extend([i, j])\n",
    "            cols.extend([j, i])\n",
    "            data.extend([score*q, score*q])\n",
    "    \n",
    "    D = scipy.sparse.coo_matrix(\n",
    "        (data, (rows, cols)),\n",
    "        shape=(len(node_list), len(node_list))\n",
    "    )\n",
    "    \n",
    "    if D.data.size > 0:\n",
    "        max_score = D.data.max()\n",
    "        D_norm = D.multiply(1.0 / max_score)\n",
    "    else:\n",
    "        D_norm = D.copy()\n",
    "    as_range = np.linspace(0, 1, 51)\n",
    "    results = []\n",
    "    \n",
    "    for a in tqdm(as_range, desc=f\"Grid Search {period}\", unit=\"step\"):\n",
    "        res = evaluate_weights_ab(a,\n",
    "                                  link_info,\n",
    "                                  D_norm,\n",
    "                                  A,\n",
    "                                  net,\n",
    "                                  sampling=0)\n",
    "        results.append(res)\n",
    "    \n",
    "    pareto_front = non_dominated_sort(results)\n",
    "    \n",
    "    print(\"\\nPareto-optimal (a, b) with (UD_auc, LCC_auc):\")\n",
    "    for a, b, ua, lcc in sorted(pareto_front, key=lambda x: x[0]):\n",
    "        print(f\"a={a:.2f}, b={b:.2f}  →  UD_auc={ua:.4f}, LCC_auc={lcc:.4f}\")\n",
    "    \n",
    "    a_list, ud_auc, lcc_auc = [], [], []\n",
    "    for a, b, ua, lcc in sorted(pareto_front, key=lambda x: x[0]):\n",
    "        a_list.append(a)\n",
    "        ud_auc.append(ua)\n",
    "        lcc_auc.append(lcc)\n",
    "    \n",
    "    a_list = np.array(a_list)\n",
    "    ud_auc = np.array(ud_auc)\n",
    "    lcc_auc = np.array(lcc_auc)\n",
    "    \n",
    "    knee_a = plot_pareto_knee(ud_auc, lcc_auc, a_list)\n",
    "    based_path = r'E:\\SUMO\\Paper1-data\\research_data\\29May_Thur_revison'\n",
    "    df_parato, saved_file = save_results_to_csv(\n",
    "        a_list, ud_auc, lcc_auc, knee_a,\n",
    "        f'{based_path}\\\\{period}\\\\pareto_param.csv'\n",
    "    )\n",
    "    \n",
    "\n",
    "    GAdj_acces = nx.to_scipy_sparse_array(net)\n",
    "    W_sp = A*knee_a + D_norm*(1-knee_a)\n",
    "    GAdj = W_sp\n",
    "    GAdj_quanlity = csr_array(D_norm)\n",
    "    \n",
    "\n",
    "    filename_curve0 = f'{based_path}\\\\{period}\\\\all_curve_param.csv'\n",
    "    filename_attack0 = f'{based_path}\\\\{period}\\\\all_attackstrategy_param.csv'\n",
    "    filename_dfod0 = f'{based_path}\\\\{period}\\\\all_domi_mergerd_OD.csv'\n",
    "    sigma0, area_ud0, area_lcc0 = main(GAdj, net, link_info, filename_curve0, filename_attack0, filename_dfod0, df_od, directed=False, analytical=False)\n",
    "    \n",
    "    filename_curve1 = f'{based_path}\\\\{period}\\\\quanlity_curve_param.csv'\n",
    "    filename_attack1 = f'{based_path}\\\\{period}\\\\quanlity_attackstrategy_param.csv'\n",
    "    filename_dfod1 = f'{based_path}\\\\{period}\\\\quanlity_domi_mergerd_OD.csv'\n",
    "    sigma1,area_ud1, area_lcc1 = main(GAdj_quanlity, net, link_info, filename_curve1, filename_attack1, filename_dfod1, df_od, directed=False, analytical=False)\n",
    "    \n",
    "    filename_curve2 = f'{based_path}\\\\{period}\\\\curve_param.csv'\n",
    "    filename_attack2 = f'{based_path}\\\\{period}\\\\attackstrategy_param.csv'\n",
    "    filename_dfod2 = f'{based_path}\\\\{period}\\\\domi_mergerd_OD.csv'\n",
    "    sigma2,area_ud2, area_lcc2 = main(GAdj_acces, net, link_info, filename_curve2, filename_attack2, filename_dfod2, df_od, directed=False, analytical=False)\n",
    "\n",
    "    result_dict['ud'].append(area_ud0)\n",
    "    result_dict['lcc'].append(area_lcc0)\n",
    "    result_dict['knee'].append(knee_a)\n",
    "    result_dict['sigma'].append(sigma0)\n",
    "    result_dict['alpha'].append(alpha) \n",
    "    result_dict['rho'].append(rho) \n",
    "\n",
    "result_dict_df = pd.DataFrame(result_dict)\n",
    "filename_all = f'{based_path}\\\\result_dict_df.csv'\n",
    "result_dict_df.to_csv(filename_all, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800d84c",
   "metadata": {},
   "source": [
    "Resilience Linear Gain Interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from heapq import heappush, heappop\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def analyze_top10_resilience_improvement(net, true_OD, total_demand, output_csv):\n",
    "\n",
    "    converted_true_OD = {(str(o), str(d)): f for (o, d), f in true_OD.items()}\n",
    "    valid_true_OD = {od: f for od, f in converted_true_OD.items() if od[0] in net and od[1] in net}\n",
    "\n",
    "    crit0 = compute_criticality(net, valid_true_OD, total_demand)\n",
    "\n",
    "    top_links = sorted(crit0.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    all_records = []\n",
    "    for rank, (link, s0) in enumerate(top_links, start=1):\n",
    "        u0, v0 = link\n",
    "        q0 = net[u0][v0]['q']\n",
    "\n",
    "        second_vals = []\n",
    "        for Orig in set(o for o, _ in valid_true_OD.keys()):\n",
    "            _, pred, paths = maximum_capacity_paths(net, Orig, weight='q')\n",
    "            for od, f in valid_true_OD.items():\n",
    "                if Orig != od[0] or od[1] not in pred: continue\n",
    "                if canonical(*pred[od[1]]) != link: continue\n",
    "                qs = [net[a][b]['q'] for a, b in zip(paths[od[1]], paths[od[1]][1:])]\n",
    "                uniq = sorted(set(qs))\n",
    "                if len(uniq) >= 2 and uniq[1] != q0:\n",
    "                    second_vals.append(uniq[1])\n",
    "\n",
    "        if second_vals:\n",
    "            bound_min = min(second_vals)\n",
    "            delta = bound_min - q0\n",
    "            q2_list = [bound_min - 1e-7] + [bound_min + delta * m for m in (1,3,5,7,10)]\n",
    "        else:\n",
    "            q2_list = [q0]\n",
    "  \n",
    "        for q2 in q2_list:\n",
    "            q2_val = max(q0, q2)\n",
    "         \n",
    "            new_q = {canonical(u,v): data['q'] for u,v,data in net.edges(data=True)}\n",
    "            new_q[link] = q2_val\n",
    "          \n",
    "            net2 = net.copy()\n",
    "            for (u,v), qq in new_q.items(): net2[u][v]['q'] = qq\n",
    "        \n",
    "            crit2 = compute_criticality(net2, valid_true_OD, total_demand)\n",
    "            # alpha0与alpha2\n",
    "            alpha0 = sum(net[u][v]['q'] * crit0.get(canonical(u,v),0) for u,v in net.edges())\n",
    "            alpha2 = sum(\n",
    "                new_q.get(canonical(u,v), net[u][v]['q']) * crit2.get(canonical(u,v), 0)\n",
    "                for u, v in net.edges()\n",
    "            )\n",
    "            s_new = crit2.get(link, 0.0)\n",
    " \n",
    "            s_old = crit0[link]\n",
    "\n",
    "            all_records.append({\n",
    "                'rank': rank,\n",
    "                'link': link,\n",
    "                'original_q': q0,\n",
    "                'new_q': q2_val,\n",
    "                'delta_q': q2_val - q0,\n",
    "                'alpha0': alpha0,\n",
    "                'alpha2': alpha2,\n",
    "                'delta_alpha': alpha2 - alpha0,\n",
    "                'gain_rate': (alpha2 - alpha0)/(q2_val - q0) if q2_val>q0 else 0.0,\n",
    "                's_old':s_old,\n",
    "                's_new':s_new\n",
    "            })\n",
    "\n",
    "        all_records.append({})\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Results written to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
